{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode, expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "#from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "\n",
    "#filename for training data\n",
    "fileName = \"trainingdata.json\"\n",
    "training = open(fileName,\"a+\") # we open file once with parameter \"a+\" that means apppend mode, when the file is exist instead of re-writing it, it will add new records to the end of file\n",
    "\n",
    "#filename for testing data\n",
    "fileNameT = \"testingdata.json\"\n",
    "testing = open(fileNameT,\"a+\")\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=5, minCount=0, inputCol=\"filtered\", outputCol=\"features\")\n",
    "\n",
    "trainingTC = 0\n",
    "numberTraining = 700\n",
    "numberTesting = 300\n",
    "testingTC = 0\n",
    "isTraining = 1\n",
    "isPrediction = 0\n",
    "model =''\n",
    "counter=0\n",
    "#thedata=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listen to Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    #event function when some data is streamed from Twitter\n",
    "    def on_data(self,data):\n",
    "        try:\n",
    "            tweet = json.loads(data)\n",
    "            text = tweet[\"text\"]\n",
    "            language_t =tweet[\"lang\"]\n",
    "            user_id = tweet[\"user\"][\"id\"]\n",
    "            date = tweet[\"user\"][\"created_at\"]\n",
    "            location= tweet[\"user\"][\"location\"]\n",
    "            \n",
    "            #create tweet json object\n",
    "            jsonObject = getJson(text, language_t, user_id, date, location)\n",
    "            \n",
    "            #saveToFile\n",
    "            saveToFile(jsonObject)\n",
    "            \n",
    "        except:\n",
    "            text = \"NONE\"\n",
    "        return True \n",
    "    def on_error(self,status):\n",
    "            print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainData(cleanData): \n",
    "    global model\n",
    "    global word2Vec\n",
    "    \n",
    "    model = word2Vec.fit(cleanData)\n",
    "    result = model.transform(cleanData)\n",
    "    result.coalesce(1).write.json(\"file:///Users/Laith/Downloads/liveTrainwordcloud.json\")\n",
    "    k = 20\n",
    "    kmeans = KMeans().setK(k).setSeed(1)\n",
    "    model = kmeans.fit(result.cache())\n",
    "    cerror = model.computeCost(result.cache())\n",
    "    print(\"Error:\")\n",
    "    print(cerror)\n",
    "    print(\"-----\")\n",
    "    centers = model.clusterCenters()\n",
    "    for center in centers:\n",
    "        print(center)\n",
    "    \n",
    "def predictData(testData):\n",
    "    global model\n",
    "    global stream\n",
    "    global word2Vec \n",
    "    print(testData.count())\n",
    "    stream.disconnect()\n",
    "    \n",
    "   \n",
    "    testmodel = word2Vec.fit(testData)\n",
    "    result = testmodel.transform(testData)\n",
    "    \n",
    "    predictions = model.transform(result.cache())\n",
    "    print(predictions.groupBy(\"prediction\").count().sort(col(\"count\").desc()).show())\n",
    "    print(\"DONE!\")\n",
    "    \n",
    "    #df = predictions.select(\"*\").where(\"prediction == 0 OR prediction == 5 OR prediction == 12 OR prediction == 2\")\n",
    "    predictions.coalesce(1).write.json(\"file:///Users/Laith/Downloads/liveTestwordcloud.json\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(jsonObject):\n",
    "    global isPrediction\n",
    "    #Language Cleaning\n",
    "    jsonObject = jsonObject.where(\"tweet_lang=='en'\")\n",
    "    jsonObject = jsonObject.drop(\"tweet_lang\")\n",
    "    #Spammers Cleaning\n",
    "    jsonObject = jsonObject.where(\"date NOT LIKE '%2019' OR date NOT LIKE '%Dec%'\")\n",
    "    jsonObject = jsonObject.where(\"date NOT LIKE '%2019' OR date NOT LIKE '%Nov%'\")\n",
    "    jsonObject = jsonObject.drop('date')\n",
    "    \n",
    "    #Location Cleaning\n",
    "    jsonObject = jsonObject.select(\"*\").where(\"location LIKE '%UK%' OR location LIKE '%England%' OR location LIKE '%Ireland%' OR location LIKE '%Scotland%' OR location LIKE '%Wales%'\")\n",
    "    #Text Cleaning\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "    regexTokenized = regexTokenizer.transform(jsonObject)\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    cleanData=remover.transform(regexTokenized) \n",
    "    if isPrediction == 1:\n",
    "        print(\"Train Data\")\n",
    "        print(\"----------\")\n",
    "        trainData(cleanData)\n",
    "    else:\n",
    "        print(\"Predict Clusters!\")\n",
    "        print(\"----------\")\n",
    "        predictData(cleanData)\n",
    "   \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData():\n",
    "    global isPrediction\n",
    "    global thedata\n",
    "    if isPrediction==1:\n",
    "        tweetsDF = spark.read.json(\"file:///Users/Laith/Downloads/trainingdata.json\")\n",
    "        print(\"Cleaning Training Data\")\n",
    "        print(\"----------\")\n",
    "        cleanData(tweetsDF)\n",
    "    else:\n",
    "        tweetsDF = spark.read.json(\"file:///Users/Laith/Downloads/testingdata.json\")\n",
    "        #print(thedata)\n",
    "        #tweetsDF = spark.read.text(thedata)\n",
    "        print(\"Cleaning Testing Data\")\n",
    "        print(\"----------\")\n",
    "        cleanData(tweetsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJson(text, language_t, user_id, date, location):\n",
    "        \n",
    "        jsonObject = {}\n",
    "        #info on the tweet\n",
    "        jsonObject[\"text\"] = text\n",
    "        jsonObject[\"tweet_lang\"] = language_t\n",
    "        jsonObject[\"userId\"]=user_id\n",
    "        jsonObject[\"date\"] = date\n",
    "        jsonObject[\"location\"]=location\n",
    "        return json.dumps(jsonObject,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(jsonObject):\n",
    "    global trainging\n",
    "    global testing\n",
    "    global stream\n",
    "    global trainingTC\n",
    "    global numberTraining\n",
    "    global numberTesting\n",
    "    global testingTC\n",
    "    global isTraining\n",
    "    global isPrediction\n",
    "   # global thedata\n",
    "    \n",
    "    \n",
    "    if isTraining==1:\n",
    "        if trainingTC<numberTraining:\n",
    "            if trainingTC == 0:\n",
    "                trainingData = jsonObject\n",
    "            else:\n",
    "                trainingData =\"\\n\"+ str(jsonObject)\n",
    "            training.write(trainingData)\n",
    "            trainingTC = trainingTC + 1\n",
    "        else:\n",
    "            print(\"Done Collecting Training Data\")\n",
    "            print(\"----------\")\n",
    "            isTraining = 0\n",
    "            isPrediction = 1\n",
    "            training.close()\n",
    "            prepareData()\n",
    "    if isPrediction==1:\n",
    "        if testingTC<numberTesting:\n",
    "            if testingTC == 0:\n",
    "                #thedata = str(jsonObject) \n",
    "                testingData = jsonObject\n",
    "            else:\n",
    "                #thedata +=\"\\n\"\n",
    "                #thedata +=str(jsonObject) \n",
    "                testingData =\"\\n\"+ str(jsonObject)\n",
    "            testing.write(testingData)\n",
    "            testingTC = testingTC + 1\n",
    "        else:\n",
    "            print(\"Done Collecting Testing Data\")\n",
    "            print(\"----------\")\n",
    "            isTraining = 0\n",
    "            isPrediction = 0\n",
    "            testing.close()\n",
    "            prepareData()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Collecting Training Data\n",
      "----------\n",
      "Cleaning Training Data\n",
      "----------\n",
      "Train Data\n",
      "----------\n",
      "Error:\n",
      "0.5063632462220918\n",
      "-----\n",
      "[-0.00675108 -0.00963421 -0.02127034  0.01712254  0.04468667]\n",
      "[ 0.35265676 -0.23838489 -0.00085204 -0.01983175  0.54192808]\n",
      "[ 0.02390559 -0.04388993 -0.06147818 -0.02891521 -0.15553067]\n",
      "[ 0.07084669 -0.08673587 -0.17668829 -0.13037618 -0.47756792]\n",
      "[ 0.05102493 -0.10541393 -0.04352352  0.07681057  0.27517457]\n",
      "[ 0.21635589 -0.17140996 -0.06887425  0.00842998  0.46643025]\n",
      "[ 0.05442541 -0.07964405 -0.12153011  0.00188124  0.04876418]\n",
      "[ 0.02669956 -0.04768196 -0.09399693 -0.06851498 -0.25620242]\n",
      "[ 0.06389806 -0.07907865 -0.05199772  0.00561332  0.11501992]\n",
      "[ 0.04222893 -0.04004376 -0.02959105  0.01462069  0.04852524]\n",
      "[ 0.14538063 -0.10207865 -0.04279329  0.05143618  0.28413789]\n",
      "[ 0.04943365 -0.06304328 -0.07063565 -0.01523985 -0.04247486]\n",
      "[ 0.02259919 -0.04760888 -0.04058219  0.02770113  0.02861752]\n",
      "[ 0.02586812 -0.05570877 -0.03858688  0.02776076  0.11628708]\n",
      "[-0.0063873  -0.10990778 -0.01446896  0.10212507  0.21490136]\n",
      "[ 0.0720847  -0.08156195 -0.04513945  0.04717544  0.08984244]\n",
      "[ 0.01935433 -0.036678   -0.04105767 -0.01758333 -0.10634354]\n",
      "[ 0.03180364 -0.02331484 -0.03909245 -0.01674664 -0.07076406]\n",
      "[ 0.00357272 -0.02111401 -0.01306687  0.00695061 -0.01256665]\n",
      "[ 0.055092   -0.06473479 -0.09098903 -0.0228174  -0.11431118]\n",
      "Done Collecting Testing Data\n",
      "----------\n",
      "Cleaning Testing Data\n",
      "----------\n",
      "Predict Clusters!\n",
      "----------\n",
      "148\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|   62|\n",
      "|        18|   54|\n",
      "|         9|   13|\n",
      "|         8|    7|\n",
      "|        12|    6|\n",
      "|        11|    5|\n",
      "|        17|    1|\n",
      "+----------+-----+\n",
      "\n",
      "None\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "stream = Stream(auth, l)\n",
    "\n",
    "#This line filter Twitter Streams to capture data by the keywords: 'Brexit'\n",
    "stream.filter(track=['Brexit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
